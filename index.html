<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
</head>
<body>
    <h2>INSTALL THESE</h2>
    <xmp>
pip install numpy
pip install matplotlib
pip install seaborn
pip install tensorflow
pip install scikit-learn
pip install pillow

    </xmp>
    <h1>EX1 HOUSING Regression and classification   </h1>
    <xmp>
!pip install seaborn

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import seaborn as sns
import matplotlib.pyplot as plt
df=pd.read_csv("Housing.csv")

df.head()
df.shape

df.describe() 
df.columns
df["price"].unique()
df["price"].unique().sum()

df.replace([np.inf, -np.inf], np.nan, inplace=True)

df.dropna(subset=['price'], inplace=True)

sns.set_style("whitegrid")

plt.figure(figsize=(10, 6))
sns.scatterplot(x='area', y='price', data=df, hue='bedrooms', palette='viridis')
plt.title('Price vs Area')
plt.xlabel('Area (sq ft)')
plt.ylabel('Price')
plt.legend(title='Bedrooms')
plt.show()

plt.figure(figsize=(12, 8))
sns.histplot(df['price'], bins=30, kde=True, color='blue', edgecolor='black')
plt.title('Distribution of House Prices', fontsize=20, weight='bold')
plt.xlabel('Price', fontsize=15)
plt.ylabel('Frequency', fontsize=15)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
# Adding a grid
plt.grid(True, linestyle='--', alpha=0.7)
# Display the plot
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x='bedrooms', data=df, palette='viridis')
plt.title('Number of Bedrooms')
plt.xlabel('Bedrooms')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x='bathrooms', data=df, palette='viridis')
plt.title('Number of Bathrooms')
plt.xlabel('Bathrooms')
plt.ylabel('Count')
plt.show()

df = df[['price', 'area', 'bedrooms', 'bathrooms']]

X = df[['area', 'bedrooms', 'bathrooms']]
y = df['price']

X.shape

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()

model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_pred    

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
    
print(f"Mean Squared Error: {mse}")
print(f"R-Squared: {r2}")    

plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test, y=y_pred)
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Actual vs Predicted Prices (Linear Regression)')
plt.show()

    </xmp>
    <h1>ex 2 ‚Å†Kmeans </h1>
    <xmp>
!pip install matplotlib
!pip install pandas
!pip install seaborn
!pip install scikit-learn

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from scipy import stats
print("pandas version: {}".format(pd.__version__))
print("numpy version: {}".format(np.__version__))
print("seaborn version: {}".format(sns.__version__))
mall_data = pd.read_csv('Mall_Customers.csv')
print('There are {} rows and {} columns in our dataset.'.format(mall_data.shape[0],mall_data.shape[1]))
mall_data.sample(10)

males_age = mall_data[mall_data['Gender']=='Male']['Age'] # subset with males age
females_age = mall_data[mall_data['Gender']=='Female']['Age'] # subset with females age 
age_bins = range(15,75,5)
# males histogram
fig2, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5), sharey=True)
sns.distplot(males_age, bins=age_bins, kde=False, color='#0066ff', ax=ax1, hist_kws=dict(edgecolor="k", linewidth=2))
ax1.set_xticks(age_bins)
ax1.set_ylim(top=25)
ax1.set_title('Males')
ax1.set_ylabel('Count')
ax1.text(45,23, "TOTAL count: {}".format(males_age.count()))
ax1.text(45,22, "Mean age: {:.1f}".format(males_age.mean()))
# females histogram
sns.distplot(females_age, bins=age_bins, kde=False, color='#cc66ff', ax=ax2, hist_kws=dict(edgecolor="k", linewidth=2))
ax2.set_xticks(age_bins)
ax2.set_title('Females')
ax2.set_ylabel('Count')
ax2.text(45,23, "TOTAL count: {}".format(females_age.count()))
ax2.text(45,22, "Mean age: {:.1f}".format(females_age.mean()))
plt.show()    

def labeler(pct, allvals):
    absolute = int(pct/100.*np.sum(allvals))
    return "{:.1f}%\n({:d})".format(pct, absolute)
sizes = [males_age.count(),females_age.count()] # wedge sizes
fig0, ax1 = plt.subplots(figsize=(6,6))
wedges, texts, autotexts = ax1.pie(sizes,
                                   autopct=lambda pct: labeler(pct, sizes),
                                   radius=1,
                                   colors=['#0066ff','#cc66ff'],
                                   startangle=90,
                                   textprops=dict(color="w"),
                                   wedgeprops=dict(width=0.7, edgecolor='w'))
ax1.legend(wedges, ['male','female'],
           loc='center right',
           bbox_to_anchor=(0.7, 0, 0.5, 1))
plt.text(0,0, 'TOTAL\n{}'.format(mall_data['Age'].count()),
         weight='bold', size=12, color='#52527a',
         ha='center', va='center')
plt.setp(autotexts, size=12, weight='bold')
ax1.axis('equal')  # Equal aspect ratio
plt.show()

males_spending = mall_data[mall_data['Gender']=='Male']['Spending Score (1-100)'] # subset with males age
females_spending = mall_data[mall_data['Gender']=='Female']['Spending Score (1-100)'] # subset with females age
spending_bins = range(0,105,5)
# males histogram
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18,5))
sns.distplot(males_spending, bins=spending_bins, kde=False, color='#0066ff', ax=ax1, hist_kws=dict(edgecolor="k", linewidth=2))
ax1.set_xticks(spending_bins)
ax1.set_xlim(0,100)
ax1.set_yticks(range(0,17,1))
ax1.set_ylim(0,16)
ax1.set_title('Males')
ax1.set_ylabel('Count')
ax1.text(50,15, "Mean spending score: {:.1f}".format(males_spending.mean()))
ax1.text(50,14, "Median spending score: {:.1f}".format(males_spending.median()))
ax1.text(50,13, "Std. deviation score: {:.1f}".format(males_spending.std()))
# females histogram
sns.distplot(females_spending, bins=spending_bins, kde=False, color='#cc66ff', ax=ax2, hist_kws=dict(edgecolor="k", linewidth=2))
ax2.set_xticks(spending_bins)
ax2.set_xlim(0,100)
ax2.set_yticks(range(0,17,1))
ax2.set_ylim(0,16)
ax2.set_title('Females')
ax2.set_ylabel('Count')
ax2.text(50,15, "Mean spending score: {:.1f}".format(females_spending.mean()))
ax2.text(50,14, "Median spending score: {:.1f}".format(females_spending.median()))
ax2.text(50,13, "Std. deviation score: {:.1f}".format(females_spending.std()))
# boxplot
sns.boxplot(x='Gender', y='Spending Score (1-100)', data=mall_data, ax=ax3)
ax3.set_title('Boxplot of spending score')
plt.show()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
# Assuming mall_data is already defined and loaded
X_numerics = mall_data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]  # subset with numeric variables only
KM_5_clusters = KMeans(n_clusters=5, init='k-means++').fit(X_numerics)  # initialise and fit K-Means model
KM5_clustered = X_numerics.copy()
KM5_clustered['Cluster'] = KM_5_clusters.labels_  # append labels to points
fig1, axes = plt.subplots(1, 2, figsize=(12, 5))
# Scatter plot for 'Annual Income (k$)' vs 'Spending Score (1-100)'
sns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)', data=KM5_clustered,
                hue='Cluster', ax=axes[0], palette='Set1', legend='full')
# Scatter plot for 'Age' vs 'Spending Score (1-100)'
sns.scatterplot(x='Age', y='Spending Score (1-100)', data=KM5_clustered,
                hue='Cluster', palette='Set1', ax=axes[1], legend='full')
# Plot cluster centers
axes[0].scatter(KM_5_clusters.cluster_centers_[:, 1], KM_5_clusters.cluster_centers_[:, 2], marker='s', s=100, c='blue', label='Centers')
axes[1].scatter(KM_5_clusters.cluster_centers_[:, 0], KM_5_clusters.cluster_centers_[:, 2], marker='s', s=100, c='blue', label='Centers')
# Adding legends to the axes
axes[0].legend()
axes[1].legend()
plt.show()

    </xmp>
    <h1>ex3 Basic image classification using CNN</h1>
    <xmp>
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from sklearn.metrics import confusion_matrix
from tensorflow.keras.preprocessing.image import load_img, img_to_array
import numpy as np
from sklearn.model_selection import train_test_split

import os
for i in os.walk("/content/drive/MyDrive/{drive folder}") :
  myimgs = i[2]
  print(i)

myimgs

X = []
for i in myimgs :
  img = load_img('/content/drive/MyDrive/Camera Roll/'+i)
  img = img_to_array(img)
  X.append(img)
X = np.array(X)
Y = np.array([1,2,0])

x_train, y_train, x_test, y_test = train_test_split(X,Y,test_size=0.2)

import matplotlib.pyplot as plt
plt.imshow(img[:,:,0])


        
# Define the LeNet model with dropout
def create_lenet_with_dropout_model():
    model = Sequential([
        Conv2D(6, (5, 5), activation='relu', input_shape=(28, 28, 1)),
        MaxPooling2D(pool_size=(2, 2)),
        Conv2D(16, (5, 5), activation='relu'),
        MaxPooling2D(pool_size=(2, 2)),
        Flatten(),
        Dense(120, activation='relu'),
        Dropout(0.5),  # Dropout layer
        Dense(84, activation='relu'),
        Dense(10, activation='softmax')
    ])
    return model
        
# Compile and train the model
model = create_lenet_with_dropout_model()
model.compile(optimizer=Adam(learning_rate=0.001), 
                loss='sparse_categorical_crossentropy', 
                metrics=['accuracy'])
        
history = model.fit(x_train, y_train, 
                    epochs=10, 
                    batch_size=64, 
                    validation_split=0.2)
        
# Plot training and validation accuracy/loss
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
        
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()
        
# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"Test accuracy: {test_acc}")
print(f"Test loss: {test_loss}")
        
# Generate confusion matrix
y_pred = np.argmax(model.predict(x_test), axis=-1)
cm = confusion_matrix(y_test, y_pred)
        
# Plot confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=np.arange(10), yticklabels=np.arange(10))
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()
        
    </xmp>
    <h1>ex4 Using lenet architecture for image classification</h1>
    <xmp>
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing import image
        
# Load VGG16 model pre-trained on ImageNet
model = VGG16(weights='imagenet', include_top=False)
        
# Load and preprocess an image
img_path = 'image.jpg'  # Replace with your image path
img = image.load_img(img_path, target_size=(224, 224))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array /= 255.0  # Scale to [0, 1]
        
# Get the output of the first convolutional layer
layer_outputs = model.layers[1].output
feature_map_model = tf.keras.Model(inputs=model.input, outputs=layer_outputs)
        
# Get feature maps
feature_maps = feature_map_model.predict(img_array)
        
# Visualize the first few feature maps
num_feature_maps = feature_maps.shape[-1]
plt.figure(figsize=(12, 12))
for i in range(min(6, num_feature_maps)):  # Display first 6 feature maps
    plt.subplot(2, 3, i + 1)
    plt.imshow(feature_maps[0, :, :, i], cmap='viridis')
    plt.axis('off')
plt.suptitle('Feature Maps from First Convolutional Layer')
plt.show()
        
# Get the weights of the first convolutional layer
filters, biases = model.layers[1].get_weights()
        
# Normalize filter values to 0-1 for better visualization
filters = (filters - filters.min()) / (filters.max() - filters.min())
        
# Plot the filters
plt.figure(figsize=(12, 12))
for i in range(min(6, filters.shape[3])):  # Display first 6 filters
    plt.subplot(2, 3, i + 1)
    plt.imshow(filters[:, :, :, i], cmap='viridis')
    plt.axis('off')
plt.suptitle('Filters from First Convolutional Layer')
plt.show()
        
    </xmp>
    <h1>ex 5 Using lenet architecture for image classification</h1>
    <xmp>
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
        
# Load the CIFAR-10 dataset
(X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()
        
# Normalize pixel values to be between 0 and 1
X_train, X_test = X_train / 255.0, X_test / 255.0
        
# Define the class names for CIFAR-10
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 
                'dog', 'frog', 'horse', 'ship', 'truck']
        
# Plot a sample image (optional)
plt.imshow(X_train[0])
plt.title(class_names[y_train[0][0]])
plt.show()
        
# Building the CNN model
model = models.Sequential()
        
# First convolutional layer
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D((2, 2)))
        
# Second convolutional layer
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
        
# Third convolutional layer
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
        
# Flatten the results to feed into a Dense layer
model.add(layers.Flatten())
        
# Fully connected layers
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10))  # 10 output classes for CIFAR-10
        
# Print the model architecture
model.summary()
        
# Compile the model
model.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])
        
# Train the model
history = model.fit(X_train, y_train, epochs=10, 
                    validation_data=(X_test, y_test))
        
# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)
print(f'\nTest accuracy: {test_acc:.4f}')
        
# Plot training and validation accuracy over epochs (optional)
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()
    </xmp>
    <h1>ex6 Vgg16 for feature extraction</h1>
    <xmp>
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import Model
        
# Load VGG16 model pre-trained on ImageNet without the top layer
base_model = VGG16(weights='imagenet', include_top=False)
        
# Function to preprocess an image
def preprocess_image(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array /= 255.0  # Scale to [0, 1]
    return img_array
        
# Function to extract features
def extract_features(img_path):
    img_array = preprocess_image(img_path)
    features = base_model.predict(img_array)
    return features
        
# Load and preprocess an image
img_path = 'horse.jpg'  # Replace with your image path
features = extract_features(img_path)
        
# Print the shape of the feature map
print("Feature map shape:", features.shape)
        
# Visualize the feature map
num_feature_maps = features.shape[-1]
plt.figure(figsize=(12, 12))
for i in range(min(6, num_feature_maps)):  # Display first 6 feature maps
    plt.subplot(2, 3, i + 1)
    plt.imshow(features[0, :, :, i], cmap='viridis')
    plt.axis('off')
plt.suptitle('Feature Maps from VGG16')
plt.show()
        
    </xmp>
    <h2>ex1 cnn binary classification</h2>
    <xmp>
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Define paths for the two folders
horse_dir = 'path/to/your/horses/folder'
human_dir = 'path/to/your/humans/folder'

# Parameters
img_height, img_width = 128, 128  # Resize images to this size
initial_lr = 0.001  # Initial learning rate
batch_size = 32
epochs = 10

# Load images and create arrays
def load_images_from_folder(folder, label, img_height, img_width):
    images = []
    labels = []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        img = tf.keras.preprocessing.image.load_img(img_path, target_size=(img_height, img_width))
        img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0  # Normalize to [0, 1]
        images.append(img_array)
        labels.append(label)
    return np.array(images), np.array(labels)

# Load horse images (label 0) and human images (label 1)
horse_images, horse_labels = load_images_from_folder(horse_dir, 0, img_height, img_width)
human_images, human_labels = load_images_from_folder(human_dir, 1, img_height, img_width)

# Combine and shuffle data
X = np.concatenate([horse_images, human_images], axis=0)
y = np.concatenate([horse_labels, human_labels], axis=0)

# Split dataset into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Visualize some samples
fig, axes = plt.subplots(2, 5, figsize=(12, 6))
axes = axes.ravel()
for i in range(10):
    axes[i].imshow(X[i])
    axes[i].set_title("Horse" if y[i] == 0 else "Human")
    axes[i].axis('off')
plt.show()

# Data augmentation
train_datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True
)
test_datagen = ImageDataGenerator()

# Create data generators
train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)
test_generator = test_datagen.flow(X_test, y_test, batch_size=batch_size)

# Custom CNN Model
def build_custom_cnn(input_shape=(img_height, img_width, 3)):
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.5),  # Dropout to prevent overfitting
        layers.Dense(1, activation='sigmoid')
    ])
    return model

# Initialize the model
model = build_custom_cnn()

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=initial_lr),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model with callbacks to monitor performance
history = model.fit(
    train_generator,
    steps_per_epoch=len(X_train) // batch_size,
    epochs=epochs,
    validation_data=test_generator,
    validation_steps=len(X_test) // batch_size
)

# Plot training and validation accuracy/loss per epoch
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.show()

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(test_generator)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")
print(f"Test Loss: {test_loss:.4f}")

# Generate predictions and create confusion matrix
y_pred = (model.predict(X_test) > 0.5).astype("int32").reshape(-1)
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Horse", "Human"], yticklabels=["Horse", "Human"])
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix")
plt.show()

# Classification Report
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=["Horse", "Human"]))

# Experiment with different hyperparameters
# Test with a different learning rate, batch size, and number of neurons
experiment_lr = 0.0005
experiment_batch_size = 64
experiment_neurons = 64

# Rebuild the model with different hyperparameters
experiment_model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(experiment_neurons, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(1, activation='sigmoid')
])

# Compile the experimental model
experiment_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=experiment_lr),
                         loss='binary_crossentropy',
                         metrics=['accuracy'])

# Train the experimental model
experiment_history = experiment_model.fit(
    train_generator,
    steps_per_epoch=len(X_train) // experiment_batch_size,
    epochs=epochs,
    validation_data=test_generator,
    validation_steps=len(X_test) // experiment_batch_size
)

# Evaluate the experimental model
experiment_test_loss, experiment_test_accuracy = experiment_model.evaluate(test_generator)
print(f"Experiment Test Accuracy: {experiment_test_accuracy * 100:.2f}%")
print(f"Experiment Test Loss: {experiment_test_loss:.4f}")

    </xmp>
    <h2>ex 2 pre-trained model and compare</h2>
    <xmp>
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models
from tensorflow.keras.applications import VGG16, ResNet50
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Define paths for the two folders
horse_dir = 'path/to/your/horses/folder'
human_dir = 'path/to/your/humans/folder'

# Parameters
img_height, img_width = 224, 224  # Required input size for VGG16 and ResNet50
initial_lr = 0.001  # Initial learning rate
batch_size = 32
epochs = 10

# Load images and create arrays
def load_images_from_folder(folder, label, img_height, img_width):
    images = []
    labels = []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        img = tf.keras.preprocessing.image.load_img(img_path, target_size=(img_height, img_width))
        img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0  # Normalize to [0, 1]
        images.append(img_array)
        labels.append(label)
    return np.array(images), np.array(labels)

# Load horse images (label 0) and human images (label 1)
horse_images, horse_labels = load_images_from_folder(horse_dir, 0, img_height, img_width)
human_images, human_labels = load_images_from_folder(human_dir, 1, img_height, img_width)

# Combine and shuffle data
X = np.concatenate([horse_images, human_images], axis=0)
y = np.concatenate([horse_labels, human_labels], axis=0)

# Split dataset into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Visualize some samples
fig, axes = plt.subplots(2, 5, figsize=(12, 6))
axes = axes.ravel()
for i in range(10):
    axes[i].imshow(X[i])
    axes[i].set_title("Horse" if y[i] == 0 else "Human")
    axes[i].axis('off')
plt.show()

# Data augmentation
train_datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True
)
test_datagen = ImageDataGenerator()

# Create data generators
train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)
test_generator = test_datagen.flow(X_test, y_test, batch_size=batch_size)

# Function to fine-tune and build a model with a pre-trained base
def build_fine_tuned_model(base_model, num_neurons=128):
    base_model.trainable = True  # Fine-tune by unfreezing all layers
    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dense(num_neurons, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=initial_lr),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model

# Load and fine-tune pre-trained VGG16 model
vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))
vgg_model = build_fine_tuned_model(vgg_base, num_neurons=128)

# Load and fine-tune pre-trained ResNet50 model
resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))
resnet_model = build_fine_tuned_model(resnet_base, num_neurons=128)

# Train VGG16 model
print("Training VGG16 Model:")
vgg_history = vgg_model.fit(
    train_generator,
    steps_per_epoch=len(X_train) // batch_size,
    epochs=epochs,
    validation_data=test_generator,
    validation_steps=len(X_test) // batch_size
)

# Train ResNet50 model
print("Training ResNet50 Model:")
resnet_history = resnet_model.fit(
    train_generator,
    steps_per_epoch=len(X_train) // batch_size,
    epochs=epochs,
    validation_data=test_generator,
    validation_steps=len(X_test) // batch_size
)

# Function to evaluate the model and print the confusion matrix
def evaluate_model(model, X_test, y_test, model_name="Model"):
    y_pred = (model.predict(X_test) > 0.5).astype("int32").reshape(-1)
    test_loss, test_accuracy = model.evaluate(test_generator)
    print(f"{model_name} Test Accuracy: {test_accuracy * 100:.2f}%")
    print(f"{model_name} Test Loss: {test_loss:.4f}")
    
    # Confusion Matrix
    conf_matrix = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Horse", "Human"], yticklabels=["Horse", "Human"])
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.title(f"{model_name} Confusion Matrix")
    plt.show()
    
    # Classification Report
    print(f"{model_name} Classification Report:")
    print(classification_report(y_test, y_pred, target_names=["Horse", "Human"]))

# Evaluate both models
evaluate_model(vgg_model, X_test, y_test, model_name="VGG16")
evaluate_model(resnet_model, X_test, y_test, model_name="ResNet50")

# Experimenting with different hyperparameters
experiment_lr = 0.0005  # New learning rate
experiment_batch_size = 64  # New batch size
experiment_neurons = 256  # Increased neurons

# Re-build and fine-tune VGG16 with new hyperparameters
print("Training VGG16 Model with New Hyperparameters:")
vgg_model_experiment = build_fine_tuned_model(vgg_base, num_neurons=experiment_neurons)
vgg_model_experiment.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=experiment_lr),
                             loss='binary_crossentropy',
                             metrics=['accuracy'])

vgg_experiment_history = vgg_model_experiment.fit(
    train_generator,
    steps_per_epoch=len(X_train) // experiment_batch_size,
    epochs=epochs,
    validation_data=test_generator,
    validation_steps=len(X_test) // experiment_batch_size
)

# Evaluate VGG16 model with new hyperparameters
evaluate_model(vgg_model_experiment, X_test, y_test, model_name="VGG16 with New Hyperparameters")

    </xmp>
</body>
</html>
